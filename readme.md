# Tea Buds Target Detection

| å·ç§¯ç±»å‹        | ğŸ§  ä½œç”¨æœºåˆ¶ / æ•ˆæœè¯´æ˜                                                         | ğŸ§© PyTorch ç¤ºä¾‹                                                                                                                     | ğŸ“ è¾“å…¥ â†’ è¾“å‡ºå½¢çŠ¶                        | âš™ï¸ æ ¸å¿ƒå‚æ•°è¯´æ˜                                                                                                                          |
|-------------|------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|-------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|
| **æ ‡å‡†å·ç§¯**    | å·ç§¯æ ¸æå–å±€éƒ¨ç‰¹å¾ï¼ˆè¾¹ç¼˜/çº¹ç†ï¼‰ï¼Œè¾“å‡ºé€šé“æ··åˆå…¨éƒ¨è¾“å…¥é€šé“ï¼Œè¡¨è¾¾èƒ½åŠ›å¼ºä½†è®¡ç®—å¤§ã€‚                               | `nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)`                                                                            | `[B, 3, 64, 64] â†’ [B, 16, 64, 64]`  | `in_channels`ï¼šè¾“å…¥é€šé“æ•°ï¼ˆå¿…é¡»è®¾ï¼‰<br>`out_channels`ï¼šè¾“å‡ºç‰¹å¾æ•°ï¼ˆå¯è°ƒï¼Œå½±å“è¡¨è¾¾ç»´åº¦ï¼‰<br>`kernel_size`ï¼šæ„Ÿå—é‡å¤§å°ï¼ˆå¸¸ç”¨ 3ï¼‰<br>`stride`ï¼šæ­¥é•¿ï¼ˆæ§åˆ¶ä¸‹é‡‡æ ·ï¼‰<br>`padding`ï¼šæ˜¯å¦ä¿ç•™å°ºå¯¸ |
| **æ·±åº¦å¯åˆ†ç¦»å·ç§¯** | åˆ†ä¸ºä¸¤æ­¥ï¼šæ¯é€šé“ç‹¬ç«‹å·ç§¯ï¼ˆdepthwiseï¼‰+ é€šé“èåˆï¼ˆpointwise 1Ã—1ï¼‰ã€‚å¤§å¹…é™ä½ FLOPs å’Œå‚æ•°é‡ï¼Œé€‚åˆè½»é‡ç½‘ç»œã€‚   | `nn.Sequential(nn.Conv2d(16, 16, 3, padding=1, groups=16), nn.Conv2d(16, 32, 1))`                                                 | `[B, 16, 64, 64] â†’ [B, 32, 64, 64]` | `groups=Cin`ï¼šæ·±åº¦å·ç§¯ï¼ˆé€šé“åˆ†ç»„ = è¾“å…¥é€šé“ï¼‰<br>`kernel_size=3`ï¼šæå–ç©ºé—´ä¿¡æ¯<br>`1x1å·ç§¯`ï¼šè°ƒæ•´è¾“å‡ºé€šé“ï¼ˆå¯è°ƒï¼‰                                                     |
| **ç©ºæ´å·ç§¯**    | å°†å·ç§¯æ ¸â€œæ‹‰ä¼¸â€åŠ ç©ºæ´ï¼Œæ‰©å¤§æ„Ÿå—é‡ï¼Œå‚æ•°ä¸å˜ä½†èƒ½æ•æ‰è¿œç¨‹ä¿¡æ¯ï¼Œå¸¸ç”¨äºè¯­ä¹‰åˆ†å‰²ã€‚                                | `nn.Conv2d(16, 32, kernel_size=3, padding=2, dilation=2)`                                                                         | `[B, 16, 64, 64] â†’ [B, 32, 64, 64]` | `dilation`ï¼šç©ºæ´ç‡ï¼ˆè¶Šå¤§æ„Ÿå—é‡è¶Šå¤§ï¼‰<br>`padding`ï¼šåº”è®¾ä¸º `dilation` å¯¹é½ç‰¹å¾è¾¹ç•Œ<br>å…¶ä½™å‚æ•°åŒæ ‡å‡†å·ç§¯ï¼ˆå¯è°ƒï¼‰                                                        |
| **ç»„å·ç§¯**     | å°†è¾“å…¥é€šé“åˆ’åˆ†ä¸º `groups` ç»„ï¼Œå„ç»„ç‹¬ç«‹å·ç§¯ï¼Œé™ä½é€šé“é—´è€¦åˆï¼ŒèŠ‚çœè®¡ç®—ï¼Œæå‡æ•ˆç‡ã€‚                          | `nn.Conv2d(32, 32, kernel_size=3, padding=1, groups=4)`                                                                           | `[B, 32, 64, 64] â†’ [B, 32, 64, 64]` | `groups`ï¼šå†³å®šæ¯ç»„å¤šå°‘é€šé“ç‹¬ç«‹å·ç§¯ï¼ˆ`groups=1`æ˜¯æ ‡å‡†å·ç§¯ï¼‰<br>å¿…é¡»æ»¡è¶³ï¼š`in_channels % groups == 0`<br>å¯è°ƒä½†éœ€æ»¡è¶³ shape ä¸€è‡´æ€§                                     |
| **å¯å˜å½¢å·ç§¯**   | å­¦ä¹ å·ç§¯æ ¸åç§»é‡ offsetï¼Œä½¿å…¶é‡‡æ ·æ›´çµæ´»ï¼Œèƒ½é€‚åº”ç›®æ ‡å½¢å˜ã€æ—‹è½¬ç­‰å¤æ‚ç»“æ„ã€‚                               | `DeformConv2d(16, 32, kernel_size=3, padding=1)`ï¼ˆéœ€å®‰è£… mmcv æˆ– detectron2 æ‰©å±•ï¼‰                                                        | `[B, 16, 64, 64] â†’ [B, 32, 64, 64]` | `offset` ç”±é¢å¤–åˆ†æ”¯ç”Ÿæˆï¼ˆè‡ªåŠ¨å­¦ä¹ ï¼‰<br>`kernel_size/padding` åŒæ ‡å‡†å·ç§¯<br>Deformable ç‰ˆæœ¬ï¼ˆv1/v2ï¼‰è¿˜å¯å­¦ä¹  modulation scalarï¼ˆå¯è°ƒï¼‰                            |
| **1Ã—1 å·ç§¯**  | åªä½œç”¨äºé€šé“ç»´åº¦ï¼Œä¸æ”¹å˜ç©ºé—´ç»´åº¦ã€‚ç”¨äºå‡é™ç»´ã€é€šé“å‹ç¼©ã€éçº¿æ€§è½¬æ¢ã€æ®‹å·®èåˆã€‚                                | `nn.Conv2d(64, 16, kernel_size=1)`                                                                                                | `[B, 64, 64, 64] â†’ [B, 16, 64, 64]` | `in_channels/out_channels` å¯è°ƒæ§åˆ¶é€šé“å‹ç¼©/æ‰©å±•æ¯”<br>`kernel_size=1` å›ºå®šï¼ˆä¸éœ€ paddingï¼‰                                                          |
| **è½¬ç½®å·ç§¯**    | ç”¨äºä¸Šé‡‡æ ·ï¼Œåå·ç§¯æ“ä½œï¼ˆéçº¿æ€§å¯å­¦ä¹ ï¼‰ã€‚å¯æ¢å¤ç©ºé—´å°ºå¯¸ï¼Œä½†æ˜“å‡ºç°â€œæ£‹ç›˜æ•ˆåº”â€ï¼ˆéœ€æ³¨æ„ kernel/stride å¯¹é½ï¼‰ã€‚          | `nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)`                                                                             | `[B, 32, 32, 32] â†’ [B, 16, 64, 64]` | `stride` æ§åˆ¶ä¸Šé‡‡æ ·å€æ•°<br>`kernel_size` åº”ä¸ `stride` å¯¹é½ï¼ˆé€šå¸¸ç­‰äº strideï¼‰<br>`padding/output_padding` è°ƒèŠ‚è¾“å‡ºå°ºå¯¸ç²¾åº¦                                 |
| **å·ç§¯æ³¨æ„åŠ›æ¨¡å—** | åŠ å…¥å…¨å±€æˆ–å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡æ˜¾è‘—åŒºåŸŸå“åº”ï¼Œå¸¸é…åˆ 1Ã—1 å·ç§¯æ§åˆ¶é€šé“æƒé‡ï¼ˆå¦‚ SEã€CBAMï¼‰ã€‚ä¸ä¼šæ”¹å˜ç‰¹å¾å°ºå¯¸ï¼Œä»…å¢å¼ºå…³é”®é€šé“æˆ–åŒºåŸŸã€‚ | `SE = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(64, 16, 1), nn.ReLU(), nn.Conv2d(16, 64, 1), nn.Sigmoid())`<br>`x * SE(x)` | `[B, 64, 64, 64] â†’ [B, 64, 64, 64]` | `hidden_dim`ï¼ˆç“¶é¢ˆé€šé“æ•°ï¼‰æ§åˆ¶å‹ç¼©æ¯”<br>æ˜¯å¦ä½¿ç”¨ç©ºé—´/é€šé“æ³¨æ„åŠ›å¯è°ƒ<br>è¾“å‡ºä¸è¾“å…¥å°ºå¯¸ä¸€è‡´ï¼Œä»…æ”¹å˜ç‰¹å¾æ¿€æ´»åˆ†å¸ƒ                                                                  |

-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------

| ç±»å‹              | ğŸ§  ä½œç”¨æœºåˆ¶ / æ•ˆæœè¯´æ˜                                                     | âš™ï¸ å…³é”®ç‰¹æ€§               | ğŸ§© PyTorch ç¤ºä¾‹                                 | ğŸ“ è¾“å…¥ â†’ è¾“å‡ºå½¢çŠ¶                    | ğŸ›ï¸ å¯è®­ç»ƒå‚æ•° |
|-----------------|--------------------------------------------------------------------|-----------------------|-----------------------------------------------|---------------------------------|-----------|
| **ReLU**        | å°†æ‰€æœ‰è´Ÿå€¼ç½® 0ï¼Œä¿æŒæ­£å€¼ä¸å˜ã€‚<br>âœ” å¼•å…¥éçº¿æ€§ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼Œè®­ç»ƒç¨³å®šã€‚                           | éçº¿æ€§ã€å•è¾¹æŠ‘åˆ¶              | `nn.ReLU()`<br>`F.relu(x)`                    | `[B, C, H, W] â†’ [B, C, H, W]`   | âŒ æ—        |
| **LeakyReLU**   | æ”¹è¿›ç‰ˆ ReLUï¼Œå…è®¸è´Ÿå€¼é€šè¿‡è¾ƒå°æ–œç‡æ³„æ¼ã€‚<br>âœ” ç¼“è§£ ReLU æ­»äº¡é—®é¢˜ã€‚                          | è´ŸåŒºé—´çº¿æ€§æ³„æ¼ï¼ˆå¦‚ slope=0.01ï¼‰ | `nn.LeakyReLU(0.01)`                          | `[B, C, H, W] â†’ [B, C, H, W]`   | âŒ æ—        |
| **Sigmoid**     | å°†å€¼æ˜ å°„åˆ° `[0, 1]`ï¼Œå¸¸ç”¨äºæ¦‚ç‡è¾“å‡ºæˆ–é—¨æ§æœºåˆ¶ã€‚<br>âœ” æ˜“é¥±å’Œï¼Œæ¢¯åº¦å°ï¼Œä¸é€‚åˆæ·±å±‚ç»“æ„ã€‚                 | è¾“å‡ºå‹ç¼©ã€å¹³æ»‘               | `nn.Sigmoid()`<br>`torch.sigmoid(x)`          | `[B, C, H, W] â†’ [B, C, H, W]`   | âŒ æ—        |
| **Tanh**        | è¾“å‡ºèŒƒå›´ `[-1, 1]`ï¼Œä¸­å¿ƒåŒ–çš„ Sigmoidã€‚<br>âœ” ç”¨äºéœ€è¦è´Ÿå€¼ä¿¡æ¯çš„åœºæ™¯ï¼Œå¦‚ RNNã€‚               | æœ‰ç¬¦å·å‹ç¼©                 | `nn.Tanh()`<br>`torch.tanh(x)`                | `[B, C, H, W] â†’ [B, C, H, W]`   | âŒ æ—        |
| **Softmax**     | å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œå¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡æœ€åä¸€å±‚ã€‚                                              | è¾“å‡ºæ‰€æœ‰å€¼å’Œä¸º 1ï¼ˆæŒ‰ dim å½’ä¸€ï¼‰   | `nn.Softmax(dim=1)`                           | `[B, C] â†’ [B, C]`               | âŒ æ—        |
| **BatchNorm2d** | å¯¹æ¯ä¸ªé€šé“åšå½’ä¸€åŒ–ï¼ˆå‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ï¼‰ï¼Œç„¶åçº¿æ€§å˜æ¢ï¼ˆå¯è®­ç»ƒ Î³, Î²ï¼‰ã€‚<br>âœ” åŠ é€Ÿæ”¶æ•›ã€ç¼“è§£æ¢¯åº¦çˆ†ç‚¸ã€‚å¸¸ç”¨äºå·ç§¯ä¹‹åã€‚ | é€šé“ç»´åº¦å½’ä¸€åŒ–ã€æ”¯æŒåŠ¨æ€ batch    | `nn.BatchNorm2d(num_features=64)`             | `[B, 64, H, W] â†’ [B, 64, H, W]` | âœ… æœ‰ï¼ˆÎ³, Î²ï¼‰ |
| **LayerNorm**   | å¯¹æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰é€šé“ + ç©ºé—´å½’ä¸€åŒ–ï¼Œå¸¸ç”¨äº NLP æˆ– transformerã€‚                          | è¾“å…¥ç»´åº¦çµæ´»ï¼Œè·¨é€šé“å½’ä¸€          | `nn.LayerNorm([C, H, W])`                     | `[B, C, H, W] â†’ [B, C, H, W]`   | âœ… æœ‰ï¼ˆÎ³, Î²ï¼‰ |
| **GroupNorm**   | å°†é€šé“åˆ†æˆ G ç»„åšå½’ä¸€åŒ–ï¼Œé€‚ç”¨äº batch size å¾ˆå°æ—¶ã€‚<br>âœ” æ›¿ä»£ BatchNormï¼Œæ›´ç¨³å®šã€‚           | æ—  batch ä¾èµ–ï¼Œå¯è·¨è®¾å¤‡ç¨³å®šè¿è¡Œ   | `nn.GroupNorm(num_groups=8, num_channels=64)` | `[B, 64, H, W] â†’ [B, 64, H, W]` | âœ… æœ‰ï¼ˆÎ³, Î²ï¼‰ |
| **Dropout**     | éšæœºå°†éƒ¨åˆ†ç¥ç»å…ƒç½®é›¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚<br>âœ” è®­ç»ƒæ—¶å¯ç”¨ï¼Œæµ‹è¯•æ—¶å…³é—­ã€‚                                | éšæœºæŠ‘åˆ¶ã€æå‡æ³›åŒ–èƒ½åŠ›           | `nn.Dropout(p=0.5)`                           | `[B, D] â†’ [B, D]`               | âŒ æ—        |

# Tea Bud Image Datasets

This repository provides three datasets â€” **A**, **B**, and **C** â€” designed for research on tea bud detection,
classification.  
Each dataset contains high-resolution images of tea buds captured under different lighting and environmental conditions,
suitable for training and evaluating computer vision models.

## Dataset Overview

- **Dataset A**: The most complex, consisting of raw images directly captured from real-world environments without special processing, containing a large number of small objects.
- **Dataset B**: Moderately difficult, with most objects being of medium size and some small objects present.
- **Dataset C**: The simplest, primarily containing medium to large objects, which are relatively easy to recognize.

## Access

You can download the datasets from the following link:

ğŸ”— **Download Link1ï¼ˆAï¼ŒBï¼ŒCï¼‰:** [Baidu Cloud](https://pan.baidu.com/s/1UgKeorPjWE6YxlakTMkOow)  
ğŸ”‘ **Extraction Code:** `r8ya`

ğŸ”— **Download Link2ï¼ˆBï¼‰:** [Roboflow](https://universe.roboflow.com/ycy-9asp0/tearob)

ğŸ”— **Download Link3ï¼ˆCï¼‰:** [Roboflow](https://universe.roboflow.com/project-n7lcw/object-detection-for-tea-bud)

# Environment and Usage Instructions

This section describes the recommended system environment, software dependencies, and usage workflow for training and
evaluating tea bud recognition models.

## 1. System Requirements

- **Operating System:** Windows 10/11, Ubuntu 20.04+
- **Hardware Configuration:**
    - GPU: NVIDIA RTX 5070 or higher (â‰¥12 GB VRAM recommended)
    - CPU: Intel i7 / AMD Ryzen 7 or better
    - RAM: â‰¥32 GB
    - Storage: â‰¥100 GB of free disk space

## 2. Software Environment

- **Programming Language:** Python 3.10
- **Deep Learning Framework:** PyTorch = 2.9.0
- **Essential Libraries:**
    - OpenCV = 4.12.0.88
    - albumentations â‰¥ 2.0.8
    - NumPy
    - Pandas
    - Matplotlib
- **Annotation Tools (optional):** LabelImg, X-AnyLabeling

## 3. Installation and Setup

```bash
# Clone the project repository
git clone https://github.com/hulonghe/buds-detect.git
cd buds-detect

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate   # (Linux/macOS)
venv\Scripts\activate      # (Windows)

# Install dependencies
pip install -r requirements.txt
```

## 4. Training and Evaluation Workflow

Download and extract the tea bud datasets (A, B, C). Start training:

```bash
python train_reg.py
```

Evaluate the trained model:

```bash
python validaate_reg.py
```

Training logs, metrics (accuracy, precision, recall, mAP), and model checkpoints will be saved in the ./runs directory.

## 5. Recommendations

Adjust batch size and learning rate according to GPU memory limits.
Use mixed precision training (torch.amp) to speed up convergence.

# Usage

Please cite this dataset appropriately if you use it in your research or publications.  
For academic and non-commercial use only.

# Acknowledgment and Contact

Thank you for using this dataset and environment guide.  
If you have any questions, suggestions, or collaboration requests, please feel free to contact us:

- **Author:** Wuyan  
- **Email:** [hlhcmp@gmail.com]

We appreciate your feedback and contribution to improving tea bud recognition research.